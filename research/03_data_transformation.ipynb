{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"E:/dibimbing/Portfolio/Bank-Loan-Default-Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\dibimbing\\\\Portfolio\\\\Bank-Loan-Default-Prediction'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loanDefault.constants import *\n",
    "from src.loanDefault.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.loanDefault import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Class Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def clean_and_preprocess_data(self):\n",
    "        # Read the CSV file\n",
    "        loans = pd.read_csv(self.config.data_path, low_memory=False)\n",
    "\n",
    "        # Drop null values from specified columns\n",
    "        loans.dropna(subset=['Name', 'City', 'State', 'BankState', 'NewExist', 'RevLineCr', 'LowDoc', 'DisbursementDate', 'MIS_Status'], inplace=True)\n",
    "\n",
    "        # Define a function to clean the string from integer data\n",
    "        def clean_data_str(value):\n",
    "            if isinstance(value, str):\n",
    "                return value.replace('A', '')\n",
    "            return value\n",
    "\n",
    "        loans['ApprovalFY'] = loans['ApprovalFY'].apply(clean_data_str).astype('int64')\n",
    "\n",
    "        # Remove '$', commas, and extra spaces from records in columns with dollar values that should be floats\n",
    "        loans[['DisbursementGross', 'BalanceGross', 'ChgOffPrinGr', 'GrAppv', 'SBA_Appv']] = \\\n",
    "        loans[['DisbursementGross', 'BalanceGross', 'ChgOffPrinGr', 'GrAppv', 'SBA_Appv']].apply(lambda x: x.str.replace('$', '').str.replace(',', '').str.strip())\n",
    "        \n",
    "        # Reconfigure remaining data into appropriate data types\n",
    "        loans = loans.astype({\n",
    "            'Zip': 'str',\n",
    "            'NewExist': 'int64',\n",
    "            'UrbanRural': 'str',\n",
    "            'DisbursementGross': 'float64',\n",
    "            'BalanceGross': 'float64',\n",
    "            'ChgOffPrinGr': 'float64',\n",
    "            'GrAppv': 'float64',\n",
    "            'SBA_Appv': 'float64'\n",
    "        })\n",
    "\n",
    "        # Mapping dictionary for NAICS codes\n",
    "        naics_mapping = {\n",
    "            '11': 'Agri/For/Fish/Hunt', # 'Agriculture, forestry, fishing and hunting',\n",
    "            '21': 'Min/Quar/Oil/Gas_ext', # 'Mining, quarrying, and oil and gas extraction',\n",
    "            '22': 'Utilities', # 'Utilities',\n",
    "            '23': 'Construction', # 'Construction',\n",
    "            '31': 'Manufacturing', # 'Manufacturing',\n",
    "            '32': 'Manufacturing', # 'Manufacturing',\n",
    "            '33': 'Manufacturing', # 'Manufacturing',\n",
    "            '42': 'Wholesale_trade', # 'Wholesale trade',\n",
    "            '44': 'Retail_trade', # 'Retail trade',\n",
    "            '45': 'Retail_trade', # 'Retail trade',\n",
    "            '48': 'Transp/WareH', # 'Transportation and warehousing',\n",
    "            '49': 'Transp/WareH', # 'Transportation and warehousing',\n",
    "            '51': 'Info', # 'Information',\n",
    "            '52': 'Finance/Insurance', # 'Finance and insurance',\n",
    "            '53': 'Real_est/Rental/Lease', # 'Real estate and rental and leasing',\n",
    "            '54': 'Prof/Science/Tech', # 'Professional, scientific, and technical services',\n",
    "            '55': 'Mgmt_comp_entp', # 'Management of companies and enterprises',\n",
    "            '56': 'Admin/Supp/WasteM/Remed', # 'Administrative and support and waste management and remediation services',\n",
    "            '61': 'Edu', # 'Educational services',\n",
    "            '62': 'Health/Social', # 'Health care and social assistance',\n",
    "            '71': 'Art/Entr/Rec', # 'Arts, entertainment, and recreation',\n",
    "            '72': 'Accom/Food', # 'Accommodation and food services',\n",
    "            '81': 'Other', # 'Other services (except public administration)',\n",
    "            '92': 'Pub_Admin', # 'Public administration'\n",
    "        }\n",
    "\n",
    "        # Apply NAICS mapping\n",
    "        loans['Industry'] = loans['NAICS'].astype('str').apply(lambda x: x[:2])\n",
    "        loans['IndustryCode'] = loans['NAICS'].astype('str').apply(lambda x: x[:2])\n",
    "        loans['Industry'] = loans['Industry'].map(naics_mapping)\n",
    "        loans.dropna(subset=['Industry'], inplace=True)\n",
    "\n",
    "        loans = loans[(loans['NewExist'] == 1) | (loans['NewExist'] == 2)]\n",
    "\n",
    "        # Create NewBusiness column\n",
    "        loans.loc[loans['NewExist'] == 1, 'NewBusiness'] = 0\n",
    "        loans.loc[loans['NewExist'] == 2, 'NewBusiness'] = 1\n",
    "\n",
    "        # Filter and clean RevLineCr and LowDoc columns\n",
    "        loans = loans[(loans['RevLineCr'] == 'Y') | (loans['RevLineCr'] == 'N')]\n",
    "        loans = loans[(loans['LowDoc'] == 'Y') | (loans['LowDoc'] == 'N')]\n",
    "\n",
    "        pd.set_option('future.no_silent_downcasting', True)\n",
    "        loans['RevLineCr'] = loans['RevLineCr'].replace(['N', 'Y'], [0, 1])\n",
    "        loans['LowDoc'] = loans['LowDoc'].replace(['N', 'Y'], [0, 1])\n",
    "\n",
    "        # Create IsFranchise flag\n",
    "        loans.loc[loans['FranchiseCode'] <= 1, 'IsFranchise'] = 0\n",
    "        loans.loc[loans['FranchiseCode'] > 1, 'IsFranchise'] = 1\n",
    "\n",
    "        # Convert date columns to datetime\n",
    "        loans[['ApprovalDate', 'DisbursementDate']] = loans[['ApprovalDate', 'DisbursementDate']].apply(pd.to_datetime)\n",
    "        loans['DaysToDisbursement'] = (loans['DisbursementDate'] - loans['ApprovalDate']).dt.days.astype('int64')\n",
    "\n",
    "        # Create DaysToDisbursement column calculating days between DisbursementDate and ApprovalDate\n",
    "        loans['DaysToDisbursement'] = (loans['DisbursementDate'] - loans['ApprovalDate']).dt.days\n",
    "\n",
    "        # Convert DaysToDisbursement to int64 dtype\n",
    "        loans['DaysToDisbursement'] = loans['DaysToDisbursement'].astype('int64')\n",
    "\n",
    "        # Create DisbursementMonth & DisbursementYear field for time analysis later (Great Recession categorizing)\n",
    "        loans['DisbursementMonth'] = loans['DisbursementDate'].map(lambda x: x.month)\n",
    "        loans['DisbursementYear'] = loans['DisbursementDate'].map(lambda x: x.year)\n",
    "\n",
    "        # Additional preprocessing steps (StateSame, SBA_AppvPercent, etc.)\n",
    "        loans['StateSame'] = np.where(loans['State'] == loans['BankState'], 1, 0)\n",
    "        loans['SBA_AppvPercent'] = loans['SBA_Appv'] / loans['GrAppv']\n",
    "        loans['AppvDisbursed'] = np.where(loans['DisbursementGross'] == loans['GrAppv'], 1, 0)\n",
    "        loans['RealEstate'] = np.where(loans['Term'] >= 240, 1, 0)\n",
    "        loans['GreatRecession'] = np.where(\n",
    "            (loans['DisbursementYear'] == 2007) & (loans['DisbursementMonth'] >= 12) |\n",
    "            (loans['DisbursementYear'] == 2008) |\n",
    "            (loans['DisbursementYear'] == 2009) & (loans['DisbursementMonth'] <= 6),\n",
    "            1, 0)\n",
    "\n",
    "        loans = loans.astype({\n",
    "            'IsFranchise': 'int64',\n",
    "            'NewBusiness': 'int64',\n",
    "            'RevLineCr': 'int64',\n",
    "            'LowDoc': 'int64'\n",
    "        })\n",
    "\n",
    "        # Create Default column based on MIS_Status\n",
    "        loans['Default'] = np.where(loans['MIS_Status'] == 'P I F', 0, 1)\n",
    "\n",
    "        state_mapping = {state: idx for idx, state in enumerate([\n",
    "            'IN', 'OK', 'FL', 'CT', 'NJ', 'NC', 'IL', 'RI', 'TX', 'VA',\n",
    "            'TN', 'AR', 'MN', 'MO', 'MA', 'CA', 'SC', 'LA', 'IA', 'OH',\n",
    "            'KY', 'MS', 'NY', 'MD', 'PA', 'OR', 'ME', 'KS', 'MI', 'AK',\n",
    "            'WA', 'CO', 'MT', 'WY', 'UT', 'NH', 'WV', 'ID', 'AZ', 'NV',\n",
    "            'WI', 'NM', 'GA', 'ND', 'VT', 'AL', 'NE', 'SD', 'HI', 'DE', 'DC'\n",
    "        ])}\n",
    "\n",
    "        bank_state_mapping = {state: idx for idx, state in enumerate([\n",
    "            'OH', 'IN', 'OK', 'FL', 'DE', 'SD', 'AL', 'CT', 'GA', 'OR', 'MN', 'RI', 'NC', 'TX',\n",
    "            'MD', 'NY', 'TN', 'SC', 'MS', 'MA', 'LA', 'IA', 'VA', 'CA', 'IL', 'KY', 'PA', 'MO',\n",
    "            'WA', 'MI', 'UT', 'KS', 'WV', 'WI', 'AZ', 'NJ', 'CO', 'ME', 'NH', 'AR', 'ND', 'MT',\n",
    "            'ID', 'WY', 'NM', 'DC', 'NV', 'NE', 'PR', 'HI', 'VT', 'AK', 'GU', 'AN', 'EN', 'VI'\n",
    "        ])}\n",
    "\n",
    "        # Drop unwanted columns\n",
    "        loans.drop(columns=['LoanNr_ChkDgt', 'ChgOffDate', 'Name', 'City', 'Zip', 'Bank', 'NAICS', 'MIS_Status', 'NewExist', 'FranchiseCode',\n",
    "                      'ApprovalDate', 'DisbursementDate', 'Industry'], inplace=True)\n",
    "        \n",
    "        # Encoding like base model encoding\n",
    "        loans['State'] = loans['State'].map(state_mapping)\n",
    "        loans['BankState'] = loans['BankState'].map(bank_state_mapping)\n",
    "        loans = loans.astype({\n",
    "            'IndustryCode' : 'int64',\n",
    "            'UrbanRural': 'int64'\n",
    "            })\n",
    "        \n",
    "        # Drop columns based on VIF Score calculation\n",
    "        loans.drop(columns=['UrbanRural', 'RealEstate', 'DisbursementYear', 'GrAppv', 'SBA_Appv', 'ChgOffPrinGr', 'IndustryCode'], inplace=True)\n",
    "\n",
    "        # drop null values after cleaning & processing\n",
    "        loans.dropna(subset=['DaysToDisbursement'], inplace=True)\n",
    "\n",
    "        # Return the cleaned and preprocessed loans dataframe\n",
    "        return loans\n",
    "    \n",
    "\n",
    "    \n",
    "    def transform_data(self, loans):\n",
    "        loans_tf = loans\n",
    "\n",
    "        features_transform = ['DaysToDisbursement', 'DisbursementGross',\n",
    "                            'RetainedJob', 'CreateJob', 'NoEmp', 'Term']\n",
    "        \n",
    "        loans_tf[features_transform] = np.log1p(loans_tf[features_transform])\n",
    "\n",
    "        loans_tf = pd.get_dummies(loans_tf)\n",
    "\n",
    "        loans_tf.dropna(subset=['DaysToDisbursement'], inplace=True)\n",
    "\n",
    "        # Replace infinite values with NaN\n",
    "        loans_tf.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "        # Optionally drop rows with NaN values\n",
    "        loans_tf.dropna(inplace=True)\n",
    "\n",
    "        # One-Hot Encoding for non ranking feature fields\n",
    "        loans_tf = pd.get_dummies(loans_tf, columns=['State']).astype(int)\n",
    "        loans_tf = pd.get_dummies(loans_tf, columns=['BankState']).astype(int)\n",
    "        loans_tf = pd.get_dummies(loans_tf, columns=['RevLineCr']).astype(int)\n",
    "        loans_tf = pd.get_dummies(loans_tf, columns=['LowDoc']).astype(int)\n",
    "\n",
    "        return loans_tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train_test_spliting(self, loans_cleaned):\n",
    "        X = loans_cleaned.drop(columns='Default')\n",
    "        y = loans_cleaned[['Default']]\n",
    "\n",
    "        # Scale predictors for easier training\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        # first split: train & pretest (split for validation & test)\n",
    "        X_train, X_pretest, y_train, y_pretest = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # second split: validation & test (validation for hyperparameter tuning)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_pretest, y_pretest, test_size=0.50, random_state=42)\n",
    "\n",
    "        # Save the train and test sets to CSV files\n",
    "        pd.DataFrame(X_train).to_csv(os.path.join(self.config.root_dir, \"X_train.csv\"), index=False)\n",
    "        pd.DataFrame(y_train).to_csv(os.path.join(self.config.root_dir, \"y_train.csv\"), index=False)\n",
    "        pd.DataFrame(X_val).to_csv(os.path.join(self.config.root_dir, \"X_val.csv\"), index=False)\n",
    "        pd.DataFrame(y_val).to_csv(os.path.join(self.config.root_dir, \"y_val.csv\"), index=False)\n",
    "        pd.DataFrame(X_test).to_csv(os.path.join(self.config.root_dir, \"X_test.csv\"), index=False)\n",
    "        pd.DataFrame(y_test).to_csv(os.path.join(self.config.root_dir, \"y_test.csv\"), index=False)\n",
    "\n",
    "\n",
    "       # Log detailed information about splits\n",
    "        logger.info(\"Data successfully split into training, validation, and test sets.\")\n",
    "        logger.info(f\"Training set X shape: {X_train.shape}, y shape: {y_train.shape}\")\n",
    "        logger.info(f\"Validation set X shape: {X_val.shape}, y shape: {y_val.shape}\")\n",
    "        logger.info(f\"Test set X shape: {X_test.shape}, y shape: {y_test.shape}\")\n",
    "\n",
    "        # Print summary of the splits\n",
    "        print(f\"Training data shape: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "        print(f\"Validation data shape: X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "        print(f\"Test data shape: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-02 21:11:13,092: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-02 21:11:13,094: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-10-02 21:11:13,098: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-10-02 21:11:13,099: INFO: common: created directory at: artifacts]\n",
      "[2024-10-02 21:11:13,100: INFO: common: created directory at: artifacts/data_transformation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juliu\\AppData\\Local\\Temp\\ipykernel_15884\\117241086.py:89: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  loans[['ApprovalDate', 'DisbursementDate']] = loans[['ApprovalDate', 'DisbursementDate']].apply(pd.to_datetime)\n",
      "C:\\Users\\juliu\\AppData\\Local\\Temp\\ipykernel_15884\\117241086.py:89: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  loans[['ApprovalDate', 'DisbursementDate']] = loans[['ApprovalDate', 'DisbursementDate']].apply(pd.to_datetime)\n",
      "C:\\Users\\juliu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\blocks.py:393: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = func(self.values, **kwargs)\n",
      "C:\\Users\\juliu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\blocks.py:393: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-02 21:12:31,401: INFO: 117241086: Data successfully split into training, validation, and test sets.]\n",
      "[2024-10-02 21:12:31,402: INFO: 117241086: Training set X shape: (319566, 124), y shape: (319566, 1)]\n",
      "[2024-10-02 21:12:31,403: INFO: 117241086: Validation set X shape: (68478, 124), y shape: (68478, 1)]\n",
      "[2024-10-02 21:12:31,403: INFO: 117241086: Test set X shape: (68479, 124), y shape: (68479, 1)]\n",
      "Training data shape: X_train: (319566, 124), y_train: (319566, 1)\n",
      "Validation data shape: X_val: (68478, 124), y_val: (68478, 1)\n",
      "Test data shape: X_test: (68479, 124), y_test: (68479, 1)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    clean_data = data_transformation.clean_and_preprocess_data()\n",
    "    transformed_data = data_transformation.transform_data(clean_data)\n",
    "    data_transformation.train_test_spliting(transformed_data)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
